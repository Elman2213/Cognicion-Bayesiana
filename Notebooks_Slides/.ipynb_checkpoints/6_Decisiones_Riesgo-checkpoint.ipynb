{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on PyMC3 v3.9.2\n"
     ]
    }
   ],
   "source": [
    "#Manejo de matrices y tablas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Estadistica y funciones matemáticas\n",
    "import scipy.stats as st\n",
    "from scipy.optimize import fmin\n",
    "from scipy import integrate\n",
    "from scipy.stats.mstats import mquantiles\n",
    "import statistics \n",
    "\n",
    "#Probabilistic programs\n",
    "import pymc3 as pm\n",
    "import theano.tensor as tt #NOTA: theano va a cambiar a tensorflow en PyMC4\n",
    "from theano.compile.ops import as_op\n",
    "print('Running on PyMC3 v{}'.format(pm.__version__))\n",
    "\n",
    "#Graficas\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import altair as alt\n",
    "from altair_saver import save #ademas instalar en terminal: brew cask install chromedriver\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, HBox, VBox, Layout\n",
    "from graphviz import Source, Digraph\n",
    "import dot2tex as d2t\n",
    "from latex import build_pdf\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import arviz as az\n",
    "\n",
    "#Funciones propias (tienen que estar en el mismo directorio)\n",
    "import my_fun as mf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decisiones de riesgo\n",
    "\n",
    "Santiago Alonso-Díaz, PhD <br>\n",
    "Universidad Javeriana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Empecemos con un ejercicio.\n",
    "\n",
    "Hombre de 95 años con tumor  \n",
    "- 0.9 de probabilidad es maligno \n",
    "- Si no es maligno, tiene 34.8 meses de vida\n",
    "- Si es maligno:\n",
    "    - Con radioterapia,  16.7 meses de vida\n",
    "    - Con cirugia, 0.35 prob. de morir, 0.65 de vivir 20.3 meses\n",
    "    - Sin tratamiento, 5.6 meses de vida\n",
    "    - Radioterapia o cirugia reduce tiempo de vida en 1 mes adicional\n",
    "\n",
    "Muestre que la radioterapia es el tratamiento preferido (valor esperado: 17.5 meses de vida)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Un marco general en teoría de la decisión es la idea de utilidad esperada\n",
    "\n",
    "$$ U(x) = p(x) v(x)$$\n",
    "\n",
    "Pregunta: ¿Es probabilidad lo mismo que riesgo? No, es un elemento.\n",
    "\n",
    "Pregunta 2: ¿Es riesgo lo mismo que incertidumbre? No"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ha aumentado el uso de la palabra riesgo (Li & Hertwig, 2020)\n",
    "<center><img src=\"img/6_CB/li1.svg\" width = \"400\" height = '400'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Principales asociaciones con riesgo (Li & Hertwig, 2020)\n",
    "<center><img src=\"img/6_CB/li2.svg\" width = \"400\" height = '400'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Preferencias de riesgo se relacionan en varios dominios (Dohmen, et al, 2011)\n",
    "<center><img src=\"img/6_CB/Dohmen1.png\" width = \"500\" height = '500'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\" style = \"font-size: 70px\"> ¿Qué es riesgo? </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# En riesgo CONOCEMOS la distribución de probabilidad del evento.\n",
    "# Riesgo puede obtenerse de las distribuciones de prob. (e.g. varianza)\n",
    "wD = widgets.Dropdown(options=[('Normal', 0), \n",
    "                               ('Uniform', 1), \n",
    "                               ('Log-Normal', 2)],\n",
    "                        value=0, description='Distr.: ')\n",
    "out = widgets.interactive_output(mf.dropdown_callback,{'distr': wD})\n",
    "VBox([wD, out])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\" style = \"font-size: 70px\"> ¿Actitudes al riesgo en economía? </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Actitudes al riesgo como concavidad de la función de utilidad\n",
    "# Alto valor baja probabilidad alto riesgo\n",
    "wD = widgets.FloatSlider(min = 0, max = 2, step = 0.01, value=1,\n",
    "                        description = 'Actitud')\n",
    "out = widgets.interactive_output(mf.slider_econ_risk,{'alpha': wD})\n",
    "VBox([wD, out])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\" style = \"font-size: 70px\"> ¿Actitudes al riesgo en ciencias cognitivas y del comportamiento? </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hay muchas alternativas:\n",
    "* Regret and disappointment theory (Bell, 1982, 1985; Loomes & Sugden, 1982)\n",
    "* Priority heuristic (Brandstätter, Gigerenzer, & Hertwig, 2006)\n",
    "* Transfer-of-attention exchange model (Birnbaum, 2008; Birnbaum & Chavez, 1997)\n",
    "* Decision field theory (Busemeyer & Townsend, 1993; Roe, Busemeyer, & Townsend, 2001) \n",
    "* Weighted utility theory (e.g. Fishburn, 1983)\n",
    "* Proportional difference model (González-Vallejo, 2002) \n",
    "* Decision affect theory (Mellers, 2000) \n",
    "* Dual system model of preference under risk (Mukherjee, 2010) \n",
    "* Rank-dependent expected utility theories (e.g., Quiggin, 1982)\n",
    "* Decisions by sampling (Stewart, Chater, & Brown, 2006)\n",
    "* Prospect theory (Kahneman & Tversky, 1979)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\" style = \"font-size: 70px\"> ¿Cómo medirlo? </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hay varias alternativas:\n",
    "* Auto-reportes\n",
    "    * Busqueda de sensaciones (Zuckerman, Eysenck, & Eysenck, 1978)\n",
    "    * Busqueda de aventuras (Eysenck, Pearson, Easting, & Allsopp, 1985)\n",
    "    * Impulsividad (Barratt, 1985; Eysenck et al., 1985)\n",
    "    * Otras (Frey, et al, 2017)\n",
    "* Mediciones controladas\n",
    "    * The Balloon Analogue Risk Task (Lejuez, et al, 2002)\n",
    "    * Loterías (Kahneman & Tversky, 1979)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Modelos Bayesianos Descriptivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# BART\n",
    "<center><img src=\"img/6_CB/lejuez1.png\" width = \"600\" height = '600'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/6_CB/lejuez2.png\" width = \"600\" height = '600'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Varios turnos. Diferentes colores del globo indican riesgo (no se le dice al sujeto). Cada inflada subía en una constante los dolares ganados y el riesgo de explotar.\n",
    "<center><img src=\"img/6_CB/lejuez3.png\" width = \"500\" height = '500'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ¿Qué riesgo mide BART?\n",
    "\n",
    "Lejuez, et al, 2002 tomaron otras medidas para correlacionarlas con BART\n",
    "\n",
    "* Auto reportes en otros dominios (busqueda de sensaciones, impulsividad, empatía, depresión, personalidad, adicciones a drogas, adicciones a juegos, uso de preservativos, entre otros)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Pumps óptimos (centro) \n",
    "<center><img src=\"img/6_CB/lejuez4.png\" width = \"600\" height = '600'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Resultados\n",
    "\n",
    "* Aversión al riesgo ($pumps < pumps_{optimo}$)\n",
    "* Poca variablidad en naranja y amarillo\n",
    "\n",
    "<center><img src=\"img/6_CB/lejuez5.png\" width = \"600\" height = '600'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "BART se correlaciona con autoreportes. Lejuez, et al, 2002 primero redujeron los autoreportes con PCA.\n",
    "\n",
    "PCA (ejemplo hipotético con dos medidas)\n",
    "<center><img src=\"img/6_CB/GaussianScatterPCA.svg\" width = \"200\" height = '200'></center>\n",
    "\n",
    "PCA (Lejuez)\n",
    "\n",
    "<center><img src=\"img/6_CB/lejuez6.png\" width = \"600\" height = '600'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "BART se correlacionó con las dimensiones de riesgo obtenidas con PCA\n",
    "\n",
    "<center><img src=\"img/6_CB/lejuez7.png\" width = \"500\" height = '500'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Discusión\n",
    "* BART mide preferencias de riesgo que no se capturan con autoreportes.\n",
    "* Hubo aversión al riesgo en promedio (menos pumps de lo óptimo).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Aplicación del BART: Contrafactuales\n",
    "\n",
    "\n",
    "<center><img src=\"img/6_CB/fitzgibbon1.png\" width = \"600\" height = '600'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Existencialismo ¿Qué tal si hubiera hecho esto?\n",
    "* ¿pagar para saber que hubiera pasado aún si lleva a \"dolor\"? e.g. confirmar infidelidad de su pareja\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Información no siempre es instrumental (i.e. no trae beneficios), pero satisface curiosidad.\n",
    "\n",
    "<center><img src=\"img/6_CB/fitzgibbon2.png\" width = \"500\" height = '520'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Información no instrumental incluso activa areas del cerebro de recompensa (Fitzgibbon, et al, 2020)\n",
    "\n",
    "<center><img src=\"img/6_CB/fitzgibbon3.png\" width = \"600\" height = '600'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Predicción: \n",
    "* Curiosidad contrafactual es un motivador así tenga costos (e.g. arrepentimiento).\n",
    "* La curiosidad contrafactual satisfecha induce cambios de comportamientos (e.g. preferir más riesgo) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Podemos usar BART para estudiar curiosidad contra-factual\n",
    "\n",
    "<center><img src=\"img/6_CB/fitzgibbon4.png\" width = \"500\" height = '500'></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/6_CB/fitzgibbon5.png\" width = \"700\" height = '700'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Los 5 experimentos fueron similares (menos busqueda de info. pues esa era la manipulación)\n",
    "<center><img src=\"img/6_CB/fitzgibbon6.png\" width = \"600\" height = '600'></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Los turnos con información aumentaban el arrepentimiento (within-subject). Missed opportunity es el numero de pumps adicionales que aguantaba el globo.\n",
    "\n",
    "<center><img src=\"img/6_CB/fitzgibbon7.png\" width = \"600\" height = '600'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "BART es una medida interesante ... ahora hagamos un modelo NOTA: el de wagenmakers + el implementarlo con la data de fitzgibbon (https://osf.io/mtsqv/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div align=\"center\" style = \"font-size: 70px\"> Prospect Theory </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Subjective value function prospect theory\n",
    "wAw = widgets.FloatSlider(min = 0, max = 1, step = 0.01, value=0.3,\n",
    "                        description = 'Alpha win')\n",
    "wAl = widgets.FloatSlider(min = 0, max = 1, step = 0.01, value=0.15,\n",
    "                        description = 'Alpha lose')\n",
    "wLA = widgets.FloatSlider(min = 0, max = 2, step = 0.01, value=2,\n",
    "                        description = 'Loss aversion')\n",
    "out = widgets.interactive_output(mf.slider_econ_beh_risk,\n",
    "                                 {'alpha_win': wAw,'alpha_lose': wAl, \n",
    "                                  'loss_aversion': wLA})\n",
    "left_widgets = VBox([wAw, wAl])\n",
    "right_widgets = VBox([wLA])\n",
    "top = HBox([left_widgets, right_widgets])\n",
    "VBox([top, out])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style = \"font-size: 70px\"> Percepción de probabilidades </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Weighting function prospect theory\n",
    "wT = widgets.FloatSlider(min = 0, max = 2, step = 0.01, value=0.5,\n",
    "                        description = 'Theta')\n",
    "out = widgets.interactive_output(mf.slider_econ_beh_prob, {'theta': wT})\n",
    "VBox([wT, out])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Algunos fenómenos psicológicos relacionados a probabilidades y riesgo\n",
    "* Optimismo (desmeritar riesgos negativos)\n",
    "* Gusto por el statu quo (decidir relativo a un referente)\n",
    "* Disponibilidad (prob. depende de recuerdos)\n",
    "* Hindsight (lo que ocurrió es lo probable)\n",
    "* Framing effects (la forma de presentar probabilidades importa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Los efectos se replican en varias poblaciones del mundo\n",
    "<center><img src=\"img/6_CB/ruggeri1.svg\" width = \"400\" height = '400'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Y prospect theory puede explicar alrededor del 90% de problemas DESCRITOS (por experiencia es complicado)\n",
    "<center><img src=\"img/6_CB/ruggeri2.svg\" width = \"400\" height = '400'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Medición de framing effects \n",
    "#### Situacion 1:\n",
    "Imagine que le damos 1000 dólares. Ya son suyos ¿Qué opción prefiere?\n",
    " * 50% de ganar 1000 más, 50% de ganar 0 más\n",
    " * 100% de certeza gana 500 más"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Situación 2:\n",
    "Imagine que le damos 2000 dólares. Ya son suyos ¿Qué opción prefiere?\n",
    " * 50% de perder 1000, 50% de perder 0 \n",
    " * 100% de certeza pierde 500 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Ejercicio en clase y en grupos:\n",
    "\n",
    "Expliqué Framing Effects con teoría de prospectos. Use las gráficas/formulas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div align=\"center\" style = \"font-size: 70px\"> Bayes y Prospect Theory </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"img/6_CB/Nilsson1.png\" width = \"600\" height = '600'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "¿Cuál prefiere?\n",
    "<center><img src=\"exp/6_CB/img/GP_0.png\" width = \"500\" height = '500'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"exp/6_CB/img/GP_69.png\" width = \"500\" height = '500'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"exp/6_CB/img/GP_138.png\" width = \"500\" height = '500'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hagamos el experimento en Psychopy (Notebooks_Slides/exp/6_CB/CPT.psyexp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Intro:\n",
    "* Un acercamiento no bayesiano, como MLE, necesita muchas observaciones por individuo.\n",
    "* Un acercamiento no jerárquico asume que los individuos son independientes. Sin embargo, los humanos compartimos sesgos.\n",
    "* MLE obtiene estimativos puntuales. Con Bayes obtenemos toda la distribución. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formulas de prospect theory\n",
    "\n",
    "Valor:\n",
    "\n",
    "$$\n",
    "v(x) = \\left\\{\n",
    "\\begin{aligned}\n",
    "    x^\\alpha \\; \\;\\; \\; &\\text{if} \\;\\; x\\ge 0 \\\\\n",
    "   -\\lambda(-x^\\beta)\\; \\;\\; \\; & \\text{if} \\;\\;  x< 0\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "<center><img src=\"img/6_CB/PT_val.svg\" width = \"150\" height = '150'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilidades:\n",
    "$$ w(p_x) = \\frac{p_x^c}{(p_x^c - (1-p_x^c))^{1/c}}$$\n",
    "\n",
    "$$c = \\gamma \\text{ if gain, } c = \\delta \\text{ if loss}$$\n",
    "\n",
    "<center><img src=\"img/6_CB/PT_weight.svg\" width = \"150\" height = '150'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valor Esperado\n",
    "$$V(x) = v(x)w(p_x)$$\n",
    "\n",
    "Decisión estocástica\n",
    "$$ p(A) = \\frac{1}{1+e^{\\phi(V(B)-V(A))}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ejercicio en clase y en grupos:\n",
    "\n",
    "1. Ponga las formulas de prospect theory en un DAG (directed acyclical graph). Haga el DAG en papel y lapiz. Más abajo está pero no lo vea. Este punto todo el mundo lo va a tener bien. El objetivo es que piense cómo expresar las formulas en un DAG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/6_CB/model_CPT.svg\" width = \"601\" height = '600'></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#REAL DATA. Load data and exp. info.\n",
    "gambles_A = pd.read_table(\"data/6_CB/GambleA.txt\", header=None)\n",
    "gambles_A.columns = ['Reward_1', 'Prob_1', 'Reward_2', 'Prob_2']\n",
    "gambles_A_win = gambles_A.loc[0:59,:].copy()\n",
    "gambles_A_loss = gambles_A.loc[60:119,:].copy()\n",
    "gambles_A_mix = gambles_A.loc[120:179,:].copy()\n",
    "gambles_B = pd.read_table(\"data/6_CB/GambleB.txt\", header=None)\n",
    "gambles_B.columns = ['Reward_1', 'Prob_1', 'Reward_2', 'Prob_2']\n",
    "gambles_B_win = gambles_B.loc[0:59,:].copy()\n",
    "gambles_B_loss = gambles_B.loc[60:119,:].copy()\n",
    "gambles_B_mix = gambles_B.loc[120:179,:].copy()\n",
    "Rieskamp_data = pd.read_table('data/6_CB/Rieskamp_data.txt', header=None) \n",
    "# 0: choice gamble A\n",
    "# 1: choice gamble B\n",
    "Rieskamp_data_win = Rieskamp_data.loc[0:59,:].copy()\n",
    "Rieskamp_data_loss = Rieskamp_data.loc[60:119,:].copy()\n",
    "Rieskamp_data_mix = Rieskamp_data.loc[120:179,:].copy()\n",
    "ntrials = Rieskamp_data.shape[0]\n",
    "ntrials_by_type = int(ntrials/3)\n",
    "nsubj = Rieskamp_data.shape[1]\n",
    "#print(ntrials, nsubj)\n",
    "#Rieskamp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 30)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SIMULATED DATA\n",
    "def value_fun(x, alpha, beta, lambbda):\n",
    "    #x: [reward1, reward2]; dataframe\n",
    "    #alpha, beta: risk attitudes (win, lose)\n",
    "    #lambbda: loss aversion   \n",
    "    if (x>=0).all().all():\n",
    "        return x**alpha\n",
    "    elif (x<=0).all().all():\n",
    "        return -lambbda*((-x)**beta)\n",
    "    else: #mix\n",
    "        if (x.iloc[:,0]>=0).all():\n",
    "            return pd.concat([x.iloc[:,0]**alpha, \n",
    "                             -lambbda*((-x.iloc[:,1])**beta)], axis=1)\n",
    "        else:\n",
    "            return pd.concat([-lambbda*((-x.iloc[:,0])**beta), \n",
    "                             x.iloc[:,1]**alpha], axis=1)\n",
    "            \n",
    "def weight_fun(x, p, gamma, delta):\n",
    "    #x: [reward1, reward2]; \n",
    "    #p: [prob1, prob2]; \n",
    "    #gamma, delta: non-linear prob. perception (win, lose)\n",
    "    \n",
    "    if (x>=0).all().all():\n",
    "        num = p**gamma\n",
    "        den = pd.concat([num.sum(axis=1)**(1/gamma),\n",
    "                         num.sum(axis=1)**(1/gamma)], axis=1)        \n",
    "    elif (x<=0).all().all():\n",
    "        num = p**delta\n",
    "        den = pd.concat([num.sum(axis=1)**(1/delta),\n",
    "                         num.sum(axis=1)**(1/delta)], axis=1)\n",
    "    else: #mix\n",
    "        if (x.iloc[:,0]>=0).all():\n",
    "            num = pd.concat([p.iloc[:,0]**gamma, \n",
    "                             p.iloc[:,1]**delta], axis=1)\n",
    "            den = pd.concat([num.sum(axis=1)**(1/gamma),\n",
    "                            num.sum(axis=1)**(1/delta)], axis=1)\n",
    "        else:\n",
    "            num = pd.concat([p.iloc[:,0]**delta, \n",
    "                             p.iloc[:,1]**gamma], axis=1)\n",
    "            den = pd.concat([num.sum()**(1/delta),\n",
    "                            num.sum()**(1/gamma)], axis=1)\n",
    "    \n",
    "    num.columns = ['Prob_1', 'Prob_2']\n",
    "    den.columns = ['Prob_1', 'Prob_2']\n",
    "    \n",
    "    return num/den\n",
    "\n",
    "def EV(v,w):\n",
    "    # v (value) and w (prob. weights)\n",
    "    # v and w are the output of value_fun and weight_fun\n",
    "    return pd.DataFrame(v.values*w.values).sum(axis=1)\n",
    "    \n",
    "def choice_rule(EV, luce):\n",
    "    return 1/(1+np.exp(luce*(EV.iloc[:,0] - EV.iloc[:,1]))) #prob. of EV[1]\n",
    "\n",
    "#CPT parameters\n",
    "alpha_sim = 0.88\n",
    "beta_sim = 0.88\n",
    "gamma_sim = 0.61\n",
    "delta_sim = 0.69\n",
    "lambda_sim = 2.25\n",
    "luce_sim = 0.14 #0.04: high choice noise; 0.14: medium; 0.4: low \n",
    "\n",
    "#WIN TRIALS\n",
    "#Gamble A\n",
    "v = value_fun(gambles_A_win[['Reward_1','Reward_2']], \n",
    "          alpha_sim, beta_sim, lambda_sim)\n",
    "w = weight_fun(gambles_A_win[['Reward_1','Reward_2']],\n",
    "               gambles_A_win[['Prob_1','Prob_2']],\n",
    "              gamma_sim, delta_sim)\n",
    "EV_A = EV(v, w)\n",
    "\n",
    "#Gamble B\n",
    "v = value_fun(gambles_B_win[['Reward_1','Reward_2']], \n",
    "          alpha_sim, beta_sim, lambda_sim)\n",
    "w = weight_fun(gambles_B_win[['Reward_1','Reward_2']],\n",
    "               gambles_B_win[['Prob_1','Prob_2']],\n",
    "              gamma_sim, delta_sim)\n",
    "EV_B = EV(v, w)\n",
    "\n",
    "#Choice\n",
    "EV_both = pd.DataFrame(np.array([EV_A, EV_B]).transpose())\n",
    "choice_prob = np.tile(choice_rule(EV_both,luce_sim),[nsubj,1]).transpose()\n",
    "choice_win = np.random.binomial(1,choice_prob).astype(int)\n",
    "\n",
    "\n",
    "#LOSS TRIALS\n",
    "#Gamble A\n",
    "v = value_fun(gambles_A_loss[['Reward_1','Reward_2']], \n",
    "          alpha_sim, beta_sim, lambda_sim)\n",
    "w = weight_fun(gambles_A_loss[['Reward_1','Reward_2']],\n",
    "               gambles_A_loss[['Prob_1','Prob_2']],\n",
    "              gamma_sim, delta_sim)\n",
    "EV_A = EV(v, w)\n",
    "\n",
    "#Gamble B\n",
    "v = value_fun(gambles_B_loss[['Reward_1','Reward_2']], \n",
    "          alpha_sim, beta_sim, lambda_sim)\n",
    "w = weight_fun(gambles_B_loss[['Reward_1','Reward_2']],\n",
    "               gambles_B_loss[['Prob_1','Prob_2']],\n",
    "              gamma_sim, delta_sim)\n",
    "EV_B = EV(v, w)\n",
    "\n",
    "#Choice\n",
    "EV_both = pd.DataFrame(np.array([EV_A, EV_B]).transpose())\n",
    "choice_prob = np.tile(choice_rule(EV_both,luce_sim),[nsubj,1]).transpose()\n",
    "choice_loss = np.random.binomial(1,choice_prob).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "#MIX TRIALS\n",
    "#Gamble A\n",
    "v = value_fun(gambles_A_mix[['Reward_1','Reward_2']], \n",
    "              alpha_sim, beta_sim, lambda_sim)\n",
    "w = weight_fun(gambles_A_mix[['Reward_1','Reward_2']],\n",
    "               gambles_A_mix[['Prob_1','Prob_2']],\n",
    "               gamma_sim, delta_sim)\n",
    "EV_A = EV(v, w)\n",
    "\n",
    "#Gamble B\n",
    "v = value_fun(gambles_B_mix[['Reward_1','Reward_2']], \n",
    "          alpha_sim, beta_sim, lambda_sim)\n",
    "w = weight_fun(gambles_B_mix[['Reward_1','Reward_2']],\n",
    "               gambles_B_mix[['Prob_1','Prob_2']],\n",
    "              gamma_sim, delta_sim)\n",
    "EV_B = EV(v, w)\n",
    "\n",
    "#Choice\n",
    "EV_both = pd.DataFrame(np.array([EV_A, EV_B]).transpose())\n",
    "choice_prob = np.tile(choice_rule(EV_both,luce_sim),[nsubj,1]).transpose()\n",
    "choice_mix = np.random.binomial(1,choice_prob).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [luce_N, lambda_N, delta_N, gamma_N, alpha_N, sigma_l_luce_N, mu_l_luce_N, sigma_l_lambda_N, mu_l_lambda_N, sigma_delta_N, mu_delta_N, sigma_gamma_N, mu_gamma_N, sigma_alpha_N, mu_alpha_N]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='10000' class='' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [10000/10000 49:45<00:00 Sampling 4 chains, 160 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 1_500 tune and 1_000 draw iterations (6_000 + 4_000 draws total) took 2989 seconds.\n",
      "There were 57 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The acceptance probability does not match the target. It is 0.8853724557431288, but should be close to 0.95. Try to increase the number of tuning steps.\n",
      "There were 90 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The acceptance probability does not match the target. It is 0.8887554181494363, but should be close to 0.95. Try to increase the number of tuning steps.\n",
      "There were 10 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "There were 3 divergences after tuning. Increase `target_accept` or reparameterize.\n",
      "The rhat statistic is larger than 1.2 for some parameters.\n",
      "The estimated number of effective samples is smaller than 200 for some parameters.\n"
     ]
    }
   ],
   "source": [
    "#PyMC model\n",
    "def norm_cdf(x, mean=0, std=1):\n",
    "    return (1.0 + tt.erf((x-mean) / tt.sqrt(2.0*(std**2)))) / 2.0 #cdf; (x is a normal sample)\n",
    "    #return tt.sqrt(2)*tt.erfinv(2x-1) #Probit: inv. cdf standard norm (x is a prob.).   \n",
    "\n",
    "with pm.Model() as CPT:  \n",
    "    # Here priors for the hyperdistributions are defined:\n",
    "    ### alpha (risk attitude win)\n",
    "    mu_alpha_N = pm.Normal('mu_alpha_N', 0, 1)\n",
    "    sigma_alpha_N = pm.Uniform('sigma_alpha_N', 0, 10)\n",
    "    ### beta (risk attitude lose)\n",
    "    #mu_beta_N = pm.Normal('mu_beta_N', 0, 1)\n",
    "    #sigma_beta_N = pm.Uniform('sigma_beta_N', 0, 10)\n",
    "    ### gamma (non-linearity in prob. win)\n",
    "    mu_gamma_N = pm.Normal('mu_gamma_N', 0, 1)\n",
    "    sigma_gamma_N = pm.Uniform('sigma_gamma_N', 0, 10)\n",
    "    ### delta (non-linearity in prob. lose)\n",
    "    mu_delta_N = pm.Normal('mu_delta_N', 0, 1)\n",
    "    sigma_delta_N = pm.Uniform('sigma_delta_N', 0, 10)\n",
    "    ### lambda (loss aversion)\n",
    "    mu_l_lambda_N = pm.Uniform('mu_l_lambda_N', -2.3, 1.61)\n",
    "    sigma_l_lambda_N = pm.Uniform('sigma_l_lambda_N', 0, 1.13)\n",
    "    ### luce (temperature of softmax)\n",
    "    mu_l_luce_N = pm.Uniform('mu_l_luce_N', -2.3, 1.61)\n",
    "    sigma_l_luce_N = pm.Uniform('sigma_l_luce_N', 0, 1.13)\n",
    "    \n",
    "    ## We put group-level normal's on the individual parameters.\n",
    "    ## This models alpha, beta, gamma, and delta as probitized parameters. \n",
    "    ## That is, it models parameters on the probit scale and then \n",
    "    ## puts them back to the range 0-1 with the CDF.\n",
    "    ## Lambda and luce are positive and modeled in log scale.\n",
    "    ## Each participant has unique parameter-values: \n",
    "    ## alpha, beta, gamma, delta, lambda, and luce\n",
    "    alpha_N = pm.TruncatedNormal('alpha_N', mu_alpha_N, sigma_alpha_N,\n",
    "                                 lower = -3, upper = 3,\n",
    "                                 shape = nsubj)\n",
    "    #beta_N = pm.TruncatedNormal('beta_N', mu_beta_N, sigma_beta_N,\n",
    "    #                            lower = -3, upper = 3,\n",
    "    #                            shape = nsubj)\n",
    "    gamma_N = pm.TruncatedNormal('gamma_N', mu_gamma_N, sigma_gamma_N,\n",
    "                                 lower = -3, upper = 3,\n",
    "                                 shape = nsubj)\n",
    "    delta_N = pm.TruncatedNormal('delta_N', mu_delta_N, sigma_delta_N,\n",
    "                                 lower = -3, upper = 3,\n",
    "                                 shape = nsubj)\n",
    "    lambda_N = pm.Normal('lambda_N', mu_l_lambda_N, sigma_l_lambda_N,\n",
    "                        shape = nsubj)\n",
    "    luce_N = pm.Normal('luce_N', mu_l_luce_N, sigma_l_luce_N,\n",
    "                       shape = nsubj)\n",
    "    \n",
    "    ### Put everything in the desired scale\n",
    "    ## We use cdf to bound some parameters to be in 0-1\n",
    "    alpha = pm.Deterministic('alpha', norm_cdf(alpha_N))\n",
    "    #beta = pm.Deterministic('beta', norm_cdf(beta_N))\n",
    "    beta = pm.Deterministic('beta', alpha)\n",
    "    gamma = pm.Deterministic('gamma', norm_cdf(gamma_N))\n",
    "    delta = pm.Deterministic('delta', norm_cdf(delta_N))\n",
    "    ## We exp because we assume a log. scale\n",
    "    lambd = pm.Deterministic('lambbda', tt.exp(lambda_N))\n",
    "    luce = pm.Deterministic('luce', tt.exp(luce_N))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # It is now time to define how the model should be fit to data.\n",
    "    ############ WIN TRIALS ############\n",
    "    gambless_A = gambles_A_win\n",
    "    gambless_B = gambles_B_win\n",
    "    ##GAMBLE A\n",
    "    ## subjective value of outcomes x & y in gamble A\n",
    "    reward_1 = np.tile(np.array(gambless_A['Reward_1']),(nsubj,1)).transpose()\n",
    "    reward_2 = np.tile(np.array(gambless_A['Reward_2']),(nsubj,1)).transpose()\n",
    "    v_x_a = pm.Deterministic('v_x_a', reward_1**tt.tile(alpha,(ntrials_by_type,1)))\n",
    "    v_y_a = pm.Deterministic('v_y_a', reward_2**tt.tile(alpha,(ntrials_by_type,1)))\n",
    "    \n",
    "    ## subjective prob. of outcomes x & y in gamble A\n",
    "    prob_1 = np.tile(np.array(gambless_A['Prob_1']),(nsubj,1)).transpose()\n",
    "    prob_2 = np.tile(np.array(gambless_A['Prob_2']),(nsubj,1)).transpose()\n",
    "    z_a = pm.Deterministic('z_a', prob_1**tt.tile(gamma,(ntrials_by_type,1)) + prob_2**tt.tile(gamma,(ntrials_by_type,1)))\n",
    "    den_a = pm.Deterministic('den_a', z_a**(1/tt.tile(gamma,(ntrials_by_type,1))))\n",
    "    num_x_a = pm.Deterministic('num_x_a', prob_1**tt.tile(gamma,(ntrials_by_type,1)))\n",
    "    w_x_a =  pm.Deterministic('w_x_a', num_x_a / den_a)  \n",
    "    num_y_a = pm.Deterministic('num_y_a', prob_2**tt.tile(gamma,(ntrials_by_type,1)))\n",
    "    w_y_a =  pm.Deterministic('w_y_a', num_y_a / den_a) \n",
    "       \n",
    "    ##subjective value of gamble A\n",
    "    Vf_a = pm.Deterministic('Vf_a', w_x_a * v_x_a + w_y_a * v_y_a)\n",
    "   \n",
    "\n",
    "    #GAMBLE B\n",
    "    ## subjective value of outcomes x & y in gamble B\n",
    "    reward_1 = np.tile(np.array(gambless_B['Reward_1']),(nsubj,1)).transpose()\n",
    "    reward_2 = np.tile(np.array(gambless_B['Reward_2']),(nsubj,1)).transpose()\n",
    "    v_x_b = pm.Deterministic('v_x_b', reward_1**tt.tile(alpha,(ntrials_by_type,1)))\n",
    "    v_y_b = pm.Deterministic('v_y_b', reward_2**tt.tile(alpha,(ntrials_by_type,1)))\n",
    "    \n",
    "    ## subjective prob. of outcomes x & y in gamble B\n",
    "    prob_1 = np.tile(np.array(gambless_B['Prob_1']),(nsubj,1)).transpose()\n",
    "    prob_2 = np.tile(np.array(gambless_B['Prob_2']),(nsubj,1)).transpose()\n",
    "    z_b = pm.Deterministic('z_b', prob_1**tt.tile(gamma,(ntrials_by_type,1)) + prob_2**tt.tile(gamma,(ntrials_by_type,1)))\n",
    "    den_b = pm.Deterministic('den_b', z_b**(1/tt.tile(gamma,(ntrials_by_type,1))))\n",
    "    num_x_b = pm.Deterministic('num_x_b', prob_1**tt.tile(gamma,(ntrials_by_type,1)))\n",
    "    w_x_b =  pm.Deterministic('w_x_b', num_x_b / den_b)  \n",
    "    num_y_b = pm.Deterministic('num_y_b', prob_2**tt.tile(gamma,(ntrials_by_type,1)))\n",
    "    w_y_b =  pm.Deterministic('w_y_b', num_y_b / den_b)   \n",
    "    \n",
    "    ##subjective value of gamble B\n",
    "    Vf_b = pm.Deterministic('Vf_b', w_x_b * v_x_b + w_y_b * v_y_b)\n",
    "    \n",
    "    ## Difference in value\n",
    "    #print(den)\n",
    "    dv = pm.Deterministic('D', (Vf_a - Vf_b))\n",
    "    ##likelihood \n",
    "    ## choice for gamble-pair is a Bernoulli-distribution \n",
    "    ## with p = binval \n",
    "    ## binval is luce's choice rule (akin to a softmax) \n",
    "    binval = pm.Deterministic('binval', 1/(1+tt.exp((tt.tile(luce,(ntrials_by_type,1))) * (dv)))) #prob. of B\n",
    "    #datta = np.array(Rieskamp_data_win)\n",
    "    #datta = pm.Data(\"data_win\", np.array(Rieskamp_data_win))\n",
    "    datta = pm.Data(\"data_win\", np.array(choice_win)) \n",
    "    win_obs = pm.Bernoulli('win_obs', p = binval, observed = datta)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ############ LOSS TRIALS ############\n",
    "    gambless_A = gambles_A_loss\n",
    "    gambless_B = gambles_B_loss\n",
    "    ##GAMBLE A\n",
    "    ## subjective value of outcomes x & y in gamble A\n",
    "    reward_1 = np.tile(np.array(gambless_A['Reward_1']),(nsubj,1)).transpose()\n",
    "    reward_2 = np.tile(np.array(gambless_A['Reward_2']),(nsubj,1)).transpose()\n",
    "    v_x_a_l = pm.Deterministic('v_x_a_l', (-1)*((-reward_1)**tt.tile(beta,(ntrials_by_type,1))))\n",
    "    v_y_a_l = pm.Deterministic('v_y_a_l', (-1)*((-reward_2)**tt.tile(beta,(ntrials_by_type,1))))\n",
    "\n",
    "    ## subjective prob. of outcomes x & y in gamble A\n",
    "    prob_1 = np.tile(np.array(gambless_A['Prob_1']),(nsubj,1)).transpose()\n",
    "    prob_2 = np.tile(np.array(gambless_A['Prob_2']),(nsubj,1)).transpose()\n",
    "    z_a_l = pm.Deterministic('z_a_l', prob_1**tt.tile(delta,(ntrials_by_type,1)) + prob_2**tt.tile(delta,(ntrials_by_type,1)))\n",
    "    den_a_l = pm.Deterministic('den_a_l', z_a_l**(1/tt.tile(delta,(ntrials_by_type,1))))\n",
    "    num_x_a_l = pm.Deterministic('num_x_a_l', prob_1**tt.tile(delta,(ntrials_by_type,1)))\n",
    "    w_x_a_l =  pm.Deterministic('w_x_a_l', num_x_a_l / den_a_l)  \n",
    "    num_y_a_l = pm.Deterministic('num_y_a_l', prob_2**tt.tile(delta,(ntrials_by_type,1)))\n",
    "    w_y_a_l =  pm.Deterministic('w_y_a_l', num_y_a_l / den_a_l) \n",
    "    \n",
    "    ##subjective value of gamble A\n",
    "    Vf_a_l = pm.Deterministic('Vf_a_l', w_x_a_l * v_x_a_l + w_y_a_l * v_y_a_l)\n",
    "    \n",
    "    \n",
    "    #GAMBLE B\n",
    "    ## subjective value of outcomes x & y in gamble B\n",
    "    reward_1 = np.tile(np.array(gambless_B['Reward_1']),(nsubj,1)).transpose()\n",
    "    reward_2 = np.tile(np.array(gambless_B['Reward_2']),(nsubj,1)).transpose()\n",
    "    v_x_b_l = pm.Deterministic('v_x_b_l', (-1)*((-reward_1)**tt.tile(beta,(ntrials_by_type,1))))\n",
    "    v_y_b_l = pm.Deterministic('v_y_b_l', (-1)*((-reward_2)**tt.tile(beta,(ntrials_by_type,1))))\n",
    "    \n",
    "    ## subjective prob. of outcomes x & y in gamble B\n",
    "    prob_1 = np.tile(np.array(gambless_B['Prob_1']),(nsubj,1)).transpose()\n",
    "    prob_2 = np.tile(np.array(gambless_B['Prob_2']),(nsubj,1)).transpose()\n",
    "    z_b_l = pm.Deterministic('z_b_l', prob_1**tt.tile(delta,(ntrials_by_type,1)) + prob_2**tt.tile(delta,(ntrials_by_type,1)))\n",
    "    den_b_l = pm.Deterministic('den_b_l', z_b_l**(1/tt.tile(delta, (ntrials_by_type,1))))\n",
    "    num_x_b_l = pm.Deterministic('num_x_b_l', prob_1**tt.tile(delta, (ntrials_by_type,1)))\n",
    "    w_x_b_l =  pm.Deterministic('w_x_b_l', num_x_b_l / den_b_l)  \n",
    "    num_y_b_l = pm.Deterministic('num_y_b_l', prob_2**tt.tile(delta, (ntrials_by_type,1)))\n",
    "    w_y_b_l =  pm.Deterministic('w_y_b_l', num_y_b_l / den_b_l)   \n",
    "\n",
    "    ##subjective value of gamble B\n",
    "    Vf_b_l = pm.Deterministic('Vf_b_l', w_x_b_l * v_x_b_l + w_y_b_l * v_y_b_l)\n",
    "    \n",
    "    ## Difference in value\n",
    "    #print(den)\n",
    "    dv_l = pm.Deterministic('D_l', (Vf_a_l - Vf_b_l))\n",
    "    ##likelihood \n",
    "    ## choice for gamble-pair is a Bernoulli-distribution \n",
    "    ## with p = binval \n",
    "    ## binval is luce's choice rule (akin to a softmax) \n",
    "    binval_l = pm.Deterministic('binval_l', 1/(1+tt.exp((tt.tile(luce,(ntrials_by_type,1))) * (dv_l)))) #prob. of B\n",
    "    #datta_l = np.array(Rieskamp_data_loss)\n",
    "    #datta_l = pm.Data(\"data_loss\", np.array(Rieskamp_data_loss))\n",
    "    datta_l = pm.Data(\"data_loss\", np.array(choice_loss))\n",
    "    loss_obs = pm.Bernoulli('loss_obs', p = binval_l, observed = datta_l)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ############ MIX TRIALS ############\n",
    "    gambless_A = gambles_A_mix\n",
    "    gambless_B = gambles_B_mix\n",
    "    ##GAMBLE A\n",
    "    ## subjective value of outcomes x & y in gamble A\n",
    "    reward_1 = np.tile(np.array(gambless_A['Reward_1']),(nsubj,1)).transpose()\n",
    "    reward_2 = np.tile(np.array(gambless_A['Reward_2']),(nsubj,1)).transpose()\n",
    "    v_x_a_m = pm.Deterministic('v_x_a_m', reward_1**tt.tile(alpha,(ntrials_by_type,1)))\n",
    "    v_y_a_m = pm.Deterministic('v_y_a_m', (-1*tt.tile(lambd,(ntrials_by_type,1)))*((-reward_2)**tt.tile(beta,(ntrials_by_type,1))))\n",
    "\n",
    "    ## subjective prob. of outcomes x & y in gamble A\n",
    "    prob_1 = np.tile(np.array(gambless_A['Prob_1']),(nsubj,1)).transpose()\n",
    "    prob_2 = np.tile(np.array(gambless_A['Prob_2']),(nsubj,1)).transpose()\n",
    "    z_a_m = pm.Deterministic('z_a_m', prob_1**tt.tile(gamma,(ntrials_by_type,1)) + prob_2**tt.tile(delta,(ntrials_by_type,1)))\n",
    "    den_a1_m = pm.Deterministic('den_a1_m', z_a_m**(1/tt.tile(gamma,(ntrials_by_type,1))))\n",
    "    den_a2_m = pm.Deterministic('den_a2_m', z_a_m**(1/tt.tile(delta,(ntrials_by_type,1))))\n",
    "    num_x_a_m = pm.Deterministic('num_x_a_m', prob_1**tt.tile(gamma,(ntrials_by_type,1)))\n",
    "    w_x_a_m =  pm.Deterministic('w_x_a_m', num_x_a_m / den_a1_m)  \n",
    "    num_y_a_m = pm.Deterministic('num_y_a_m', prob_2**tt.tile(delta,(ntrials_by_type,1)))\n",
    "    w_y_a_m =  pm.Deterministic('w_y_a_m', num_y_a_m / den_a2_m) \n",
    "    \n",
    "    ##subjective value of gamble A\n",
    "    Vf_a_m = pm.Deterministic('Vf_a_m', w_x_a_m * v_x_a_m + w_y_a_m * v_y_a_m)\n",
    "    \n",
    "    \n",
    "    ##GAMBLE B\n",
    "    ## subjective value of outcomes x & y in gamble B\n",
    "    reward_1 = np.tile(np.array(gambless_B['Reward_1']),(nsubj,1)).transpose()\n",
    "    reward_2 = np.tile(np.array(gambless_B['Reward_2']),(nsubj,1)).transpose()\n",
    "    v_x_b_m = pm.Deterministic('v_x_b_m', reward_1**tt.tile(alpha,(ntrials_by_type,1)))\n",
    "    v_y_b_m = pm.Deterministic('v_y_b_m', (-1*tt.tile(lambd,(ntrials_by_type,1)))*((-reward_2)**tt.tile(beta,(ntrials_by_type,1))))\n",
    "\n",
    "    ## subjective prob. of outcomes x & y in gamble B\n",
    "    prob_1 = np.tile(np.array(gambless_B['Prob_1']),(nsubj,1)).transpose()\n",
    "    prob_2 = np.tile(np.array(gambless_B['Prob_2']),(nsubj,1)).transpose()\n",
    "    z_b_m = pm.Deterministic('z_b_m', prob_1**tt.tile(gamma,(ntrials_by_type,1)) + prob_2**tt.tile(delta,(ntrials_by_type,1)))\n",
    "    den_b1_m = pm.Deterministic('den_b1_m', z_b_m**(1/tt.tile(gamma,(ntrials_by_type,1))))\n",
    "    den_b2_m = pm.Deterministic('den_b2_m', z_b_m**(1/tt.tile(delta,(ntrials_by_type,1))))\n",
    "    num_x_b_m = pm.Deterministic('num_x_b_m', prob_1**tt.tile(gamma,(ntrials_by_type,1)))\n",
    "    w_x_b_m =  pm.Deterministic('w_x_b_m', num_x_b_m / den_b1_m)  \n",
    "    num_y_b_m = pm.Deterministic('num_y_b_m', prob_2**tt.tile(delta,(ntrials_by_type,1)))\n",
    "    w_y_b_m =  pm.Deterministic('w_y_b_m', num_y_b_m / den_b2_m) \n",
    "    \n",
    "    ##subjective value of gamble B\n",
    "    Vf_b_m = pm.Deterministic('Vf_b_m', w_x_b_m * v_x_b_m + w_y_b_m * v_y_b_m)\n",
    "    \n",
    "        \n",
    "    ## Difference in value\n",
    "    #print(den)\n",
    "    dv_m = pm.Deterministic('D_m', (Vf_a_m - Vf_b_m))\n",
    "    ##likelihood \n",
    "    ## choice for gamble-pair is a Bernoulli-distribution \n",
    "    ## with p = binval \n",
    "    ## binval is luce's choice rule (akin to a softmax) \n",
    "    binval_m = pm.Deterministic('binval_m', 1/(1+tt.exp((tt.tile(luce,(ntrials_by_type,1))) * (dv_m)))) #prob. of B\n",
    "    #datta_m = np.array(Rieskamp_data_mix)\n",
    "    #datta_m = pm.Data(\"data_mix\", np.array(Rieskamp_data_mix))\n",
    "    datta_m = pm.Data(\"data_mix\", np.array(choice_mix))\n",
    "    mix_obs = pm.Bernoulli('mix_obs', p = binval_m, observed = datta_m)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##############  Sampling  ##############\n",
    "    trace = pm.sample(1000, tune = 1500, init='adapt_diag', target_accept = 0.95)\n",
    "    #step = pm.Metropolis()\n",
    "    #trace = pm.sample(50000, tune = 5000, step=step)\n",
    "    rhat = pm.rhat(trace, var_names = ['alpha', 'beta', 'gamma', 'delta', 'lambbda', 'luce'])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'alpha' ()>\n",
      "array(1.05341645)\n",
      "<xarray.DataArray 'beta' ()>\n",
      "array(1.05341645)\n",
      "<xarray.DataArray 'gamma' ()>\n",
      "array(1.00426198)\n",
      "<xarray.DataArray 'delta' ()>\n",
      "array(1.02483718)\n",
      "<xarray.DataArray 'lambbda' ()>\n",
      "array(1.0140686)\n",
      "<xarray.DataArray 'luce' ()>\n",
      "array(1.04696647)\n",
      "mu_alpha_N -0.9189385332046727\n",
      "sigma_alpha_N_interval__ -1.3862943611198906\n",
      "mu_gamma_N -0.9189385332046727\n",
      "sigma_gamma_N_interval__ -1.3862943611198906\n",
      "mu_delta_N -0.9189385332046727\n",
      "sigma_delta_N_interval__ -1.3862943611198906\n",
      "mu_l_lambda_N_interval__ -1.3862943611198906\n",
      "sigma_l_lambda_N_interval__ -1.3862943611198906\n",
      "mu_l_luce_N_interval__ -1.3862943611198906\n",
      "sigma_l_luce_N_interval__ -1.3862943611198906\n",
      "alpha_N_interval__ -39.83152842291766\n",
      "gamma_N_interval__ -39.83152842291766\n",
      "delta_N_interval__ -39.83152842291766\n",
      "lambda_N -10.440269561069291\n",
      "luce_N -10.440269561069291\n",
      "win_obs -1075.3927049013678\n",
      "loss_obs -1030.1261335676527\n",
      "mix_obs -1160.8047698331486\n"
     ]
    }
   ],
   "source": [
    "# Revisar convergencia. \n",
    "# Rhat <1.1 usualmente son aceptados como convergencia\n",
    "print(rhat.alpha[0:nsubj].mean())\n",
    "print(rhat.beta[0:nsubj].mean())\n",
    "print(rhat.gamma[0:nsubj].mean())\n",
    "print(rhat.delta[0:nsubj].mean())\n",
    "print(rhat.lambbda[0:nsubj].mean())\n",
    "print(rhat.luce[0:nsubj].mean())\n",
    "for RV in CPT.basic_RVs: #None should be inf or -inf\n",
    "    print(RV.name, RV.logp(CPT.test_point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaciones de convergencia\n",
    "pars = [('mu_alpha_N','mu_alpha_N'), ('sigma_alpha_N','sigma_alpha_N'), \n",
    "        ('mu_beta_N','mu_beta_N'),('sigma_beta_N','sigma_beta_N'), \n",
    "        ('mu_gamma_N','mu_gamma_N'), ('sigma_gamma_N','sigma_gamma_N'),\n",
    "        ('mu_delta_N','mu_delta_N'), ('sigma_delta_N','sigma_delta_N'), \n",
    "        ('mu_l_lambda_N','mu_l_lambda_N'),('sigma_l_lambda_N','sigma_l_lambda_N'), \n",
    "        ('mu_l_luce_N','mu_l_luce_N'), ('sigma_l_luce_N','sigma_l_luce_N'),\n",
    "       ('alpha_N','alpha_N'), ('beta_N','beta_N'), ('gamma_N','gamma_N'),\n",
    "        ('delta_N','delta_N'), ('lambda_N','lambda_N'), ('luce_N','luce_N'), \n",
    "        ('alpha','alpha'), ('beta','beta'),\n",
    "        ('gamma','gamma'), ('delta','delta'), \n",
    "        ('lambda','lambbda'), ('luce','luce')]\n",
    "wD = widgets.Dropdown(options=pars,\n",
    "                        value='alpha', description='Param.: ')\n",
    "out = widgets.interactive_output(mf.dropdown_convergence,{'param': wD, \n",
    "                                                       'trace': fixed(trace)})\n",
    "VBox([wD, out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alpha', 0.9525090151265312, 0.008450654368386759]\n",
      "['beta', 0.9525090151265312, 0.008450654368386759]\n",
      "['gamma', 0.5886913136943912, 0.018465526346126443]\n",
      "['delta', 0.7925871720672897, 0.01061665299261482]\n",
      "['lambbda', 2.069978585635886, 0.021255689602986915]\n",
      "['luce', 0.13386085694919814, 0.0042205205918649175]\n"
     ]
    }
   ],
   "source": [
    "print(['alpha',np.median(np.median(trace['alpha'], axis = 0)),\n",
    "       np.median(trace['alpha'], axis = 0).std()]) #columns in trace are subjects, rows samples\n",
    "print(['beta',np.median(np.median(trace['beta'], axis = 0)),\n",
    "       np.median(trace['beta'], axis = 0).std()])\n",
    "print(['gamma',np.median(np.median(trace['gamma'], axis = 0)),\n",
    "       np.median(trace['gamma'], axis = 0).std()])\n",
    "print(['delta',np.median(np.median(trace['delta'], axis = 0)),\n",
    "       np.median(trace['delta'], axis = 0).std()])\n",
    "print(['lambbda',np.median(np.median(trace['lambbda'], axis = 0)),\n",
    "       np.median(trace['lambbda'], axis = 0).std()])\n",
    "print(['luce',np.median(np.median(trace['luce'], axis = 0)),\n",
    "       np.median(trace['luce'], axis = 0).std()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Posterior plots of 6 CPT parameters (group-level)\n",
    "data = az.from_pymc3(trace=trace, model=CPT)\n",
    "fig, ax = plt.subplots(2,2, figsize = (8,8))\n",
    "az.plot_density([st.norm.cdf(trace['mu_alpha_N']),\n",
    "                st.norm.cdf(trace['mu_beta_N'])],\n",
    "                data_labels=['Alpha' , 'Beta'],\n",
    "                hdi_prob=1, ax = ax[0,0]);\n",
    "az.plot_density([st.norm.cdf(trace['mu_gamma_N']),\n",
    "                st.norm.cdf(trace['mu_delta_N'])],\n",
    "                data_labels=['Gamma' , 'Delta'],\n",
    "                hdi_prob=1, ax = ax[0,1]);\n",
    "az.plot_density(np.exp(trace['mu_l_lambda_N']),\n",
    "                data_labels=['Lambda'],\n",
    "                hdi_prob=1, ax = ax[1,0]);\n",
    "az.plot_density(np.exp(trace['mu_l_luce_N']),\n",
    "                data_labels=['Luce'],\n",
    "                hdi_prob=1, ax = ax[1,1]);\n",
    "ax[0,0].set_title('')\n",
    "ax[0,1].set_title('')\n",
    "ax[1,0].set_title('Lambda')\n",
    "ax[1,1].set_title('Luce')\n",
    "fig.savefig('img/6_CB/Nilsson_Fig3.svg')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"img/6_CB/Nilsson_Fig3.svg\" width = \"551\" height = '550'></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#az.plot_trace(data);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Plan de ruta\n",
    "1. Que es riesgo?\n",
    "2. que es riesgo en economia? Concavidad de la funcion de utilidad\n",
    "3. qué es riesgo en economía del comportamiento? prospect theory\n",
    "4. que es probabilidad en psicologia cognitiva? Representations (e.g. fractions) + Process (e.g. Bayesian updating)\n",
    "    * Discrete prob. (my ratio bias paper)\n",
    "    * Continuous prob. (BART (Wagenmakers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "NOTA: Ver capitulo de 9 de BDA de Gelman et al como introduccion. Luego empezar con el de Wagenmakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to slides --SlidesExporter.reveal_theme='solarized' --SlidesExporter.reveal_transition='none' --SlidesExporter.reveal_scroll=True 6_Decisiones_Riesgo.ipynb #Saves slide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "Para salvar las diapositivas a PDF (en Chrome), correr nbconvert para que abra las diapositivas en un servidor local (la transition y el theme son opcionales):\n",
    "\n",
    "!jupyter nbconvert --to slides --SlidesExporter.reveal_theme='solarized' --SlidesExporter.reveal_transition='convex' nombre_de_mi_notebook.ipynb --post serve\n",
    "\n",
    "Luego, a la dirección añadirle ?print-pdf después del .html:\n",
    "\n",
    "http://127.0.0.1:8000/nombre_de_mi_notebook.slides.html?print-pdf\n",
    "\n",
    "Y luego, imprimir y darle salvar como pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#Para salvar a pdf\n",
    "!jupyter nbconvert --to slides --SlidesExporter.reveal_theme='solarized' --SlidesExporter.reveal_transition='none' 6_Decisiones_Riesgo.ipynb --post serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Anexos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#CPT\n",
    "dot_text = 'digraph G {rankdir=TB; compound=true; newrank=true; labelloc=\"t\";\\\n",
    "           label=\"Cumulative Prospect Theory\";\\\n",
    "           /* general properties*/\\\n",
    "           node [margin=0, fixedsize=true, shape=plaintext,\\\n",
    "                 height=0.55, width=0.55, lblstyle=\"font=\\\\small\"];\\\n",
    "           /* links */\\\n",
    "           hyper_mu_sd -> alpha_normal;\\\n",
    "           hyper_mu_sd -> beta_normal;\\\n",
    "           hyper_mu_sd -> gamma_normal;\\\n",
    "           hyper_mu_sd -> delta_normal;\\\n",
    "           hyper_mu_sd_exp -> lambda_normal;\\\n",
    "           hyper_mu_sd_exp -> phi_normal;\\\n",
    "           subgraph cluster0 {\\\n",
    "               margin = 10; labeljust=l; lblstyle=\"font=\\\\small\";\\\n",
    "               style = rounded;\\\n",
    "               label = \" \"; texlbl = \"$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; i$ Individuo\";\\\n",
    "               alpha_normal -> alpha;\\\n",
    "               beta_normal -> beta;\\\n",
    "               gamma_normal -> gamma;\\\n",
    "               delta_normal -> delta;\\\n",
    "               phi_normal -> phi;\\\n",
    "               lambda_normal -> lambd;\\\n",
    "               alpha -> val;\\\n",
    "               lambd -> val;\\\n",
    "               beta -> val;\\\n",
    "               gamma -> w_prob;\\\n",
    "               delta -> w_prob;\\\n",
    "               phi -> choice;\\\n",
    "               subgraph cluster1 {\\\n",
    "                   margin = 10; labeljust=l; lblstyle=\"font=\\\\small\";\\\n",
    "                   style = rounded;\\\n",
    "                   label = \" \"; texlbl = \"$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; t$ Turno\";\\\n",
    "                   x -> val;\\\n",
    "                   x -> c;\\\n",
    "                   c -> w_prob;\\\n",
    "                   p -> w_prob;\\\n",
    "                   val -> EV;\\\n",
    "                   w_prob -> EV;\\\n",
    "                   EV -> choice;\\\n",
    "                   choice -> choice_data;\\\n",
    "               }\\\n",
    "           }\\\n",
    "           choice_data -> choice_data_dist -> hyper_mu_dist -> hyper_sd_dist -> hyper_mu_dist_exp -> hyper_sd_dist_exp -> alpha_normal_dist ->  alpha_dist -> beta_normal_dist -> beta_dist -> gamma_normal_dist -> gamma_dist -> delta_normal_dist -> delta_dist -> lambda_normal_dist -> lambda_dist -> phi_normal_dist -> phi_dist -> val_dist -> c_dist -> w_prob_dist -> EV_dist -> choice_dist [style = invis];\\\n",
    "           /* nodes */\\\n",
    "           hyper_mu_sd [texlbl = \"$(\\\\mu,\\\\sigma)$\", shape = circle];\\\n",
    "           hyper_mu_dist[texlbl = \"$\\\\mu \\sim Normal(0,1)$\"];\\\n",
    "           hyper_sd_dist [texlbl = \"$\\\\sigma \\sim Uniform(0,10)$\"];\\\n",
    "           hyper_mu_sd_exp [texlbl = \"$(\\\\mu_l,\\\\sigma_l)$\", shape = circle];\\\n",
    "           hyper_mu_dist_exp [texlbl = \"$\\\\mu_l \\sim Uniform(-2.3,1.61)$\"];\\\n",
    "           hyper_sd_dist_exp  [texlbl = \"$\\\\sigma_l \\sim Uniform(0,1.13)$\"];\\\n",
    "           alpha_normal [texlbl = \"$\\\\alpha_{N_i}$\", shape = circle];\\\n",
    "           alpha_normal_dist [texlbl = \"$\\\\alpha_{N_i} \\sim Normal(\\\\mu,\\\\sigma)[-3,3]$\"];\\\n",
    "           alpha [texlbl = \"$\\\\alpha_i$\", shape = circle, peripheries = 2];\\\n",
    "           alpha_dist  [texlbl = \"$\\\\alpha_i \\sim Std-Normal_{CDF}(\\\\alpha_{N_i})$\"];\\\n",
    "           beta_normal [texlbl = \"$\\\\beta_{N_i}$\", shape = circle];\\\n",
    "           beta_normal_dist [texlbl = \"$\\\\beta_{N_i} \\sim Normal(\\\\mu,\\\\sigma)[-3,3]$\"];\\\n",
    "           beta [texlbl = \"$\\\\beta_i$\", shape = circle, peripheries = 2];\\\n",
    "           beta_dist  [texlbl = \"$\\\\beta_i \\sim Std-Normal_{CDF}(\\\\beta_{N_i})$\"];\\\n",
    "           gamma_normal [texlbl = \"$\\\\gamma_{N_i}$\", shape = circle];\\\n",
    "           gamma_normal_dist [texlbl = \"$\\\\gamma_{N_i} \\sim Normal(\\\\mu,\\\\sigma)[-3,3]$\"];\\\n",
    "           gamma [texlbl = \"$\\\\gamma_i$\", shape = circle, peripheries = 2];\\\n",
    "           gamma_dist  [texlbl = \"$\\\\gamma_i \\sim Std-Normal_{CDF}(\\\\gamma_{N_i})$\"];\\\n",
    "           delta_normal [texlbl = \"$\\\\delta_{N_i}$\", shape = circle];\\\n",
    "           delta_normal_dist [texlbl = \"$\\\\delta_{N_i} \\sim Normal(\\\\mu,\\\\sigma)[-3,3]$\"];\\\n",
    "           delta [texlbl = \"$\\\\delta_i$\", shape = circle, peripheries = 2];\\\n",
    "           delta_dist  [texlbl = \"$\\\\delta_i \\sim Std-Normal_{CDF}(\\\\delta_{N_i})$\"];\\\n",
    "           lambda_normal [texlbl = \"$\\\\lambda_{N_i}$\", shape = circle];\\\n",
    "           lambda_normal_dist [texlbl = \"$\\\\lambda_{N_i} \\sim Normal(\\\\mu_l,\\\\sigma_l)$\"];\\\n",
    "           lambd [texlbl = \"$\\\\lambda_i$\", shape = circle, peripheries = 2];\\\n",
    "           lambda_dist  [texlbl = \"$\\\\lambda_i \\sim e^{\\\\lambda_{N_i}}$\"];\\\n",
    "           phi_normal [texlbl = \"$\\\\phi_{N_i}$\", shape = circle];\\\n",
    "           phi_normal_dist [texlbl = \"$\\\\phi_{N_i} \\sim Normal(\\\\mu_l,\\\\sigma_l)$\"];\\\n",
    "           phi [texlbl = \"$\\\\phi_i$\", shape = circle, peripheries = 2];\\\n",
    "           phi_dist  [texlbl = \"$\\\\phi_i \\sim e^{\\\\phi_{N_i}}$\"];\\\n",
    "           x [texlbl = \"$x_{it}$\", shape = circle, fillcolor = gray, style = filled];\\\n",
    "           p [texlbl = \"$p_{it}$\", shape = circle, fillcolor = gray, style = filled];\\\n",
    "           c [texlbl = \"$c_{it}$\", shape = circle];\\\n",
    "           c_dist [texlbl = \"$c_{it} = \\\\left\\\\lbrace \\\\parbox{7cm}{$\\\\gamma_i \\;\\;\\;\\;\\;\\; x_{it} \\ge 0$ \\\\\\ \\\\\\ $\\\\delta_i \\;\\;\\;\\;\\;\\; x_{it} <0 $} \\\\right.$\"];\\\n",
    "           w_prob [texlbl = \"$\\\\pi(p_{it})$\", shape = circle];\\\n",
    "           w_prob_dist [texlbl = \"$\\\\pi(p_{it})= \\\\frac{p_{it}^c}{(p_{it}^c-(1-p_{it}^c))^{1/c}}$\"];\\\n",
    "           val [texlbl = \"$v(x_{it})$\", shape = circle];\\\n",
    "           val_dist [texlbl = \"$v(x_{it}) = \\\\left\\\\lbrace \\\\parbox{7cm}{$x_{it}^{\\\\alpha_i} \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; x_{it} \\ge 0$ \\\\\\ \\\\\\ $-\\\\lambda_i(-x_{it})^{\\\\beta} \\;\\;\\;\\; x_{it} <0 $} \\\\right.$\"];\\\n",
    "           EV [texlbl = \"$V(O)$\", shape = circle, peripheries = 2];\\\n",
    "           EV_dist [texlbl = \"$V(O) = \\pi(p_{it_A})v(x_{it_A}) + \\pi(p_{it_B})v(x_{it_B})$\"];\\\n",
    "           choice [texlbl = \"$p_{it}(A,B)$\", shape = circle];\\\n",
    "           choice_dist [texlbl = \"$p_{it}(A,B) = \\\\frac{1}{1+e^{\\\\phi (V(B_t)-V(A_t))}}$\"];\\\n",
    "           choice_data [texlbl = \"$Choice_{it}$\", shape = square, style = filled, fillcolor = gray];\\\n",
    "           choice_data_dist [texlbl = \"$Choice_{it} \\sim Bernoulli(p_{it}(A,B))$\"];\\\n",
    "           }' #warning: use single quote at start and end; double quotes for labels\n",
    "tex = d2t.dot2tex(dot_text, format='tikz', preproc = True) #makes sure it looks good in tex\n",
    "tex = d2t.dot2tex(dot_text, texmode = 'verbatim', crop=True) #crop: the page size equal to the model\n",
    "diagram_tex = open('img/6_CB/model_CPT.tex', 'w')\n",
    "diagram_tex.write(tex) \n",
    "diagram_tex.close()\n",
    "\n",
    "# this builds a pdf-file inside a directory\n",
    "pdf = build_pdf(tex)\n",
    "pdf.save_to('img/6_CB/model_CPT.pdf') #convertir a svg y pulir/editar posiciones en inkscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPT images for trials\n",
    "# Gambles shown to participants. \n",
    "# A and B appeared on the left or right randomly\n",
    "gambles_A = pd.read_table(\"data/6_CB/GambleA.txt\", header=None)\n",
    "gambles_A.columns = ['Reward_1', 'Prob_1', 'Reward_2', 'Prob_2']\n",
    "gambles_B = pd.read_table(\"data/6_CB/GambleB.txt\", header=None)\n",
    "gambles_B.columns = ['Reward_1', 'Prob_1', 'Reward_2', 'Prob_2']\n",
    "ntrials = gambles_B.shape[0]\n",
    "file_address_psychopy = []\n",
    "for n in range(ntrials):\n",
    "    if n<60: #win trials\n",
    "        colors = ['#228B22','#20B2AA']\n",
    "        title = 'GANAR'\n",
    "    elif n>59 and n<120:\n",
    "        colors = [\"#8B0000\", '#FF0000']\n",
    "        title = 'PERDER'\n",
    "    else:\n",
    "        colors = [\"#228B22\", '#FF0000']\n",
    "        title = 'GANAR & PERDER'\n",
    "        \n",
    "    # Creating plot\n",
    "    fig = plt.figure(figsize=(12,5), constrained_layout=True)\n",
    "    spec = GridSpec(ncols=28, nrows=1, figure=fig)\n",
    "    ax0 = fig.add_subplot(spec[0, 0:1])\n",
    "    ax1 = fig.add_subplot(spec[0, 2:12])\n",
    "    ax2 = fig.add_subplot(spec[0, 13:14])\n",
    "    ax3 = fig.add_subplot(spec[0, 15:25])\n",
    "    ax4 = fig.add_subplot(spec[0, 26:27])\n",
    "    #fig, (ax1,ax2,ax3) = plt.subplots(1,3,figsize=(9,3)) #ax1,ax2 refer to your two pies\n",
    "    \n",
    "    labels = gambles_A.loc[n,['Reward_1', 'Reward_2']]*1000\n",
    "    labels = [\"${:,.0f}\".format(labels[0]), \"${:,.0f}\".format(labels[1])]\n",
    "    values = gambles_A.loc[n,['Prob_1', 'Prob_2']]\n",
    "    _, texts, autotexts = ax1.pie(values, labels = labels,\n",
    "                              colors = colors,autopct = '%1.1f%%', radius=1.5) #plot first pie\n",
    "    [ _.set_fontsize(16) for _ in texts ]\n",
    "    [ _.set_fontsize(12) for _ in autotexts ]    \n",
    "    #ax1.axis('equal')\n",
    "    ax1.set_ylim(-1,2)\n",
    "    ax1.set_title(title, fontsize = 18)\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "\n",
    "    ax0.set_axis_off()\n",
    "    ax1.set_axis_off()\n",
    "    ax2.set_axis_off()\n",
    "    ax3.set_axis_off()\n",
    "    ax4.set_axis_off()\n",
    "\n",
    "    labels = gambles_B.loc[n,['Reward_1', 'Reward_2']]*1000\n",
    "    labels = [\"${:,.0f}\".format(labels[0]), \"${:,.0f}\".format(labels[1])]\n",
    "    values = gambles_B.loc[n,['Prob_1', 'Prob_2']]\n",
    "    _, texts, autotexts = ax3.pie(values, labels = labels, \n",
    "                              colors = colors,autopct = '%1.1f%%', radius=1.5) #plot first pie\n",
    "    [ _.set_fontsize(16) for _ in texts ]\n",
    "    [ _.set_fontsize(12) for _ in autotexts ]\n",
    "    ax3.set_ylim(-1,2)\n",
    "    #ax3.axis('equal')\n",
    "    ax3.set_title(title, fontsize = 18)\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "    file_address_psychopy.append('img/GP_' + str(n) + '.png')\n",
    "    fig.savefig('exp/6_CB/img/GP_' + str(n) + '.png')\n",
    "    plt.close()     \n",
    "#bbox_inches= \"tight\"\n",
    "f.to_csv('exp/6_CB/img_links.csv')\n",
    "      \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "rise": {
   "chalkboard": {
    "color": [
     "rgb(250, 0, 0)",
     "rgb(0, 250, 250)"
    ]
   },
   "enable_chalkboard": true,
   "scroll": true,
   "theme": "simple",
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
